{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantBook Analysis Tool \n",
    "# For more information see [https://www.quantconnect.com/docs/v2/our-platform/research/getting-started]\n",
    "qb = QuantBook()\n",
    "spy = qb.AddEquity(\"SPY\")\n",
    "history = qb.History(qb.Securities.Keys, 360, Resolution.Daily)\n",
    "\n",
    "# Indicator Analysis\n",
    "bbdf = qb.Indicator(BollingerBands(30, 2), spy.Symbol, 360, Resolution.Daily)\n",
    "bbdf.drop('standarddeviation', axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AlgorithmImports import *\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_random_slices(symbol, frontier_date, ending_date, interval=Resolution.Minute, \n",
    "                        slice_length=timedelta(days=1), n_slices=5):\n",
    "    \"\"\"\n",
    "    Picks independent slices of stock movement. Each slice is:\n",
    "    random start time → start + slice_length.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol : str\n",
    "        Ticker symbol (e.g., \"AAPL\").\n",
    "    frontier_date : datetime\n",
    "        Earliest possible start date.\n",
    "    ending_date : datetime\n",
    "        Latest possible end date.\n",
    "    interval : Resolution\n",
    "        Data granularity (Minute, Hour, Daily).\n",
    "    slice_length : timedelta\n",
    "        How long each slice should be (e.g., 1 day, 3 hours).\n",
    "    n_slices : int\n",
    "        Number of random slices to return.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts { \"start_date\", \"start_price\", \"end_date\", \"end_price\" }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load all history once\n",
    "    history = qb.History([symbol], frontier_date, ending_date, interval)\n",
    "    if history.empty:\n",
    "        return []\n",
    "    \n",
    "    data = history[\"close\"].reset_index()\n",
    "    \n",
    "    results = []\n",
    "    for _ in range(n_slices):\n",
    "        # Pick random start time ensuring we can go slice_length forward\n",
    "        latest_start = ending_date - slice_length\n",
    "        if frontier_date >= latest_start:\n",
    "            break  # Not enough room for slices\n",
    "        \n",
    "        rand_start = frontier_date + timedelta(\n",
    "            seconds=random.randint(0, int((latest_start - frontier_date).total_seconds()))\n",
    "        )\n",
    "        rand_end = rand_start + slice_length\n",
    "        \n",
    "        # Find nearest available prices\n",
    "        start_row = data.loc[data[\"time\"] >= rand_start].head(1)\n",
    "        end_row   = data.loc[data[\"time\"] >= rand_end].head(1)\n",
    "        \n",
    "        if not start_row.empty and not end_row.empty:\n",
    "            results.append({\n",
    "                \"start_date\": start_row[\"time\"].iloc[0],\n",
    "                \"start_price\": float(start_row[\"close\"].iloc[0]),\n",
    "                \"end_date\": end_row[\"time\"].iloc[0],\n",
    "                \"end_price\": float(end_row[\"close\"].iloc[0])\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "qb = QuantBook()\n",
    "symbol = qb.AddEquity(\"AAPL\").Symbol\n",
    "\n",
    "frontier_date = datetime(2015, 1, 1)\n",
    "ending_date   = datetime(2025, 9, 15)\n",
    "\n",
    "slices = fetch_random_slices(\n",
    "    symbol, frontier_date, ending_date,\n",
    "    interval=Resolution.Minute,\n",
    "    slice_length=timedelta(days=3),   # 3-day window\n",
    "    n_slices=5\n",
    ")\n",
    "\n",
    "for s in slices:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AlgorithmImports import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_consecutive_slices(symbol, frontier_date, ending_date, interval=Resolution.Minute, \n",
    "                             slice_length=timedelta(days=1), max_slices=None):\n",
    "    \"\"\"\n",
    "    Fetches consecutive, non-overlapping slices of stock movement.\n",
    "    Each slice is frontier_date → frontier_date+slice_length, then continues until ending_date.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol : str\n",
    "        Ticker symbol (e.g., \"AAPL\").\n",
    "    frontier_date : datetime\n",
    "        Earliest possible start date.\n",
    "    ending_date : datetime\n",
    "        Latest possible end date.\n",
    "    interval : Resolution\n",
    "        Data granularity (Minute, Hour, Daily).\n",
    "    slice_length : timedelta\n",
    "        Length of each slice (e.g., 1 day, 3 hours).\n",
    "    max_slices : int, optional\n",
    "        Maximum number of slices to return. Default = all possible slices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts { \"start_date\", \"start_price\", \"end_date\", \"end_price\" }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load history once\n",
    "    history = qb.History([symbol], frontier_date, ending_date, interval)\n",
    "    if history.empty:\n",
    "        return []\n",
    "    \n",
    "    data = history[\"close\"].reset_index()\n",
    "    \n",
    "    results = []\n",
    "    current_start = frontier_date\n",
    "    n_slices = 0\n",
    "    \n",
    "    while current_start + slice_length <= ending_date:\n",
    "        current_end = current_start + slice_length\n",
    "        \n",
    "        # Find nearest available prices\n",
    "        start_row = data.loc[data[\"time\"] >= current_start].head(1)\n",
    "        end_row   = data.loc[data[\"time\"] >= current_end].head(1)\n",
    "        \n",
    "        if not start_row.empty and not end_row.empty:\n",
    "            results.append({\n",
    "                \"start_date\": start_row[\"time\"].iloc[0],\n",
    "                \"start_price\": float(start_row[\"close\"].iloc[0]),\n",
    "                \"end_date\": end_row[\"time\"].iloc[0],\n",
    "                \"end_price\": float(end_row[\"close\"].iloc[0])\n",
    "            })\n",
    "            n_slices += 1\n",
    "        \n",
    "        # Stop if we've hit the slice cap\n",
    "        if max_slices is not None and n_slices >= max_slices:\n",
    "            break\n",
    "        \n",
    "        # Advance to next slice\n",
    "        current_start = current_end\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def slices_to_extended_dataframe(slices, time_format=\"%Y-%m-%d|%H:%M\"):\n",
    "    \"\"\"\n",
    "    Converts slice outputs into an extended DataFrame with:\n",
    "    - linked date ranges\n",
    "    - raw price change\n",
    "    - % change\n",
    "    - log returns (%)\n",
    "    - cumulative % return (compounded)\n",
    "    - cumulative log return (%)\n",
    "    - cumulative return from logs (%)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    slices : list of dicts\n",
    "        Output from fetch_random_slices or fetch_consecutive_slices.\n",
    "    time_format : str\n",
    "        How to format the timestamps in the linked range.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: \n",
    "        [date_range, start_date, end_date, raw_change, pct_change, \n",
    "         log_return, cum_return, cum_log_return, cum_return_from_logs]\n",
    "    \"\"\"\n",
    "    if not slices:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"date_range\", \"start_date\", \"end_date\", \n",
    "            \"raw_change\", \"pct_change\", \"log_return\", \n",
    "            \"cum_return\", \"cum_log_return\", \"cum_return_from_logs\"\n",
    "        ])\n",
    "    \n",
    "    rows = []\n",
    "    for s in slices:\n",
    "        start_date = s[\"start_date\"]\n",
    "        end_date   = s[\"end_date\"]\n",
    "        start_price = s[\"start_price\"]\n",
    "        end_price   = s[\"end_price\"]\n",
    "        \n",
    "        # Format date range string\n",
    "        start_str = start_date.strftime(time_format)\n",
    "        end_str   = end_date.strftime(time_format)\n",
    "        date_range = f\"{start_str} : {end_str}\"\n",
    "        \n",
    "        # Compute changes\n",
    "        raw_change = end_price - start_price\n",
    "        pct_change = (raw_change / start_price) * 100\n",
    "        log_return = np.log(end_price / start_price) * 100  # now %\n",
    "        \n",
    "        rows.append({\n",
    "            \"date_range\": date_range,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"raw_change\": raw_change,\n",
    "            \"pct_change\": pct_change,\n",
    "            \"log_return\": log_return\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Cumulative compounded % return\n",
    "    df[\"cum_return\"] = (1 + df[\"pct_change\"]/100).cumprod() - 1\n",
    "    df[\"cum_return\"] = df[\"cum_return\"] * 100  # %\n",
    "    \n",
    "    # Cumulative additive log return (in %)\n",
    "    df[\"cum_log_return\"] = df[\"log_return\"].cumsum()\n",
    "    \n",
    "    # Convert cumulative log return into implied compounded return (%)\n",
    "    df[\"cum_return_from_logs\"] = (np.exp(df[\"cum_log_return\"]/100) - 1) * 100\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def slices_to_dataframe(slices):\n",
    "    \"\"\"\n",
    "    Converts slice output into a DataFrame with raw % change.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    slices : list of dicts\n",
    "        Output from fetch_random_slices or fetch_consecutive_slices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: [start_date, end_date, start_price, end_price, pct_change]\n",
    "    \"\"\"\n",
    "    if not slices:\n",
    "        return pd.DataFrame(columns=[\"start_date\", \"end_date\", \"start_price\", \"end_price\", \"pct_change\"])\n",
    "    \n",
    "    df = pd.DataFrame(slices)\n",
    "    df[\"pct_change\"] = (df[\"end_price\"] - df[\"start_price\"]) / df[\"start_price\"] * 100\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_slices = fetch_consecutive_slices(\n",
    "    symbol, \n",
    "    frontier_date, \n",
    "    ending_date,\n",
    "    interval=Resolution.Minute,\n",
    "    slice_length=timedelta(days=3),\n",
    "    max_slices = 45\n",
    ")\n",
    "\n",
    "consecutive_slices_df = slices_to_extended_dataframe(consecutive_slices)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for Normality\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def test_normality(df, column=\"log_return\"):\n",
    "    \"\"\"\n",
    "    Tests normality of a return column using Q-Q plot and Shapiro-Wilk test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame from slices_to_extended_dataframe.\n",
    "    column : str\n",
    "        Column to test (\"pct_change\" or \"log_return\").\n",
    "    \"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    # --- Q-Q Plot ---\n",
    "    plt.figure(figsize=(6,6))\n",
    "    stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q Plot of {column}\", fontsize=14)\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel(\"Sample Quantiles\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Shapiro-Wilk Test ---\n",
    "    stat, p = stats.shapiro(data)\n",
    "    \n",
    "    print(f\"Shapiro-Wilk Test for {column}\")\n",
    "    print(f\"  Test Statistic = {stat:.4f}\")\n",
    "    print(f\"  p-value        = {p:.4f}\")\n",
    "    \n",
    "    if p > 0.05:\n",
    "        print(\"✅ Fail to reject H0: Data looks normally distributed.\")\n",
    "    else:\n",
    "        print(\"❌ Reject H0: Data does not look normally distributed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normality Testing\n",
    "df = slices_to_extended_dataframe(slices)\n",
    "\n",
    "# Test for log returns\n",
    "test_normality(df, column=\"log_return\")\n",
    "\n",
    "# Or test for simple % changes\n",
    "test_normality(df, column=\"pct_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_slices = fetch_random_slices(\n",
    "    symbol, frontier_date, ending_date,\n",
    "    interval=Resolution.Minute,\n",
    "    slice_length=timedelta(days=3),   # 3-day window\n",
    "    n_slices=100\n",
    ")\n",
    "\n",
    "random_slice_df = slices_to_extended_dataframe(random_slices)\n",
    "\n",
    "# Test for log returns\n",
    "test_normality(random_slice_df, column=\"log_return\")\n",
    "\n",
    "# Or test for simple % changes\n",
    "test_normality(random_slice_df, column=\"pct_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Normality + Features (same as before)\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(df, column=\"log_return\"):\n",
    "    data = df[column].dropna()\n",
    "    if len(data) < 3:\n",
    "        return None, 0\n",
    "    \n",
    "    features = {\n",
    "        \"mean\": np.mean(data),\n",
    "        \"std\": np.std(data),\n",
    "        \"skewness\": skew(data),\n",
    "        \"kurtosis\": kurtosis(data),\n",
    "        \"n_obs\": len(data),\n",
    "    }\n",
    "    _, p = shapiro(data)\n",
    "    return features, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Optimization Loop\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def optimize_normality(symbol, frontier_date, ending_date, \n",
    "                       n_iter=30, init_points=5,\n",
    "                       slice_length_bounds=(1, 20),\n",
    "                       n_slices_bounds=(20, 500),\n",
    "                       column=\"log_return\"):\n",
    "    \"\"\"\n",
    "    Closed-loop optimizer to maximize normality score (Shapiro-Wilk p-value).\n",
    "    \"\"\"\n",
    "    history = []  # store results\n",
    "    \n",
    "    # Step 1: initial random exploration\n",
    "    for _ in range(init_points):\n",
    "        days = random.randint(*slice_length_bounds)\n",
    "        n_slices = random.randint(*n_slices_bounds)\n",
    "        \n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        \n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "    \n",
    "    # Step 2: adaptive loop\n",
    "    for it in range(init_points, n_iter):\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        X = hist_df.drop(columns=[\"score\"])\n",
    "        y = hist_df[\"score\"]\n",
    "        \n",
    "        # Train surrogate model\n",
    "        model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Generate candidate parameter grid\n",
    "        candidates = pd.DataFrame([\n",
    "            {\"slice_length_days\": d, \"n_slices\": n, \n",
    "             \"mean\": 0, \"std\": 0, \"skewness\": 0, \"kurtosis\": 0, \"n_obs\": 0}\n",
    "            for d in range(slice_length_bounds[0], slice_length_bounds[1]+1)\n",
    "            for n in range(n_slices_bounds[0], n_slices_bounds[1]+1, 20)\n",
    "        ])\n",
    "        \n",
    "        # Predict scores\n",
    "        candidates[\"pred_score\"] = model.predict(candidates)\n",
    "        best = candidates.sort_values(\"pred_score\", ascending=False).iloc[0]\n",
    "        \n",
    "        # Evaluate best candidate\n",
    "        days = int(best[\"slice_length_days\"])\n",
    "        n_slices = int(best[\"n_slices\"])\n",
    "        \n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        \n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "        \n",
    "        print(f\"Iteration {it+1}/{n_iter} → days={days}, n_slices={n_slices}, score={score:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Optimizer\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "def extract_features(df, column=\"log_return\"):\n",
    "    data = df[column].dropna()\n",
    "    if len(data) < 3:\n",
    "        return None, 0\n",
    "    features = {\n",
    "        \"mean\": np.mean(data),\n",
    "        \"std\": np.std(data),\n",
    "        \"skewness\": skew(data),\n",
    "        \"kurtosis\": kurtosis(data),\n",
    "        \"n_obs\": len(data),\n",
    "    }\n",
    "    _, p = shapiro(data)\n",
    "    return features, p\n",
    "\n",
    "def optimize_normality(symbol, frontier_date, ending_date, \n",
    "                       n_iter=30, init_points=5,\n",
    "                       slice_length_bounds=(1, 20),\n",
    "                       n_slices_bounds=(20, 500),\n",
    "                       column=\"log_return\"):\n",
    "    \"\"\"\n",
    "    Closed-loop optimizer to maximize normality score (Shapiro-Wilk p-value).\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    # Step 1: Initial random exploration\n",
    "    for _ in range(init_points):\n",
    "        days = random.randint(*slice_length_bounds)\n",
    "        n_slices = random.randint(*n_slices_bounds)\n",
    "        \n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        \n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "    \n",
    "    # Step 2: Adaptive loop\n",
    "    for it in range(init_points, n_iter):\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        X = hist_df.drop(columns=[\"score\"])\n",
    "        y = hist_df[\"score\"]\n",
    "        feature_cols = X.columns.tolist()\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Build candidates grid\n",
    "        candidates = pd.DataFrame([\n",
    "            {\"slice_length_days\": d, \"n_slices\": n,\n",
    "             \"mean\": 0, \"std\": 0, \"skewness\": 0, \"kurtosis\": 0, \"n_obs\": 0}\n",
    "            for d in range(slice_length_bounds[0], slice_length_bounds[1] + 1)\n",
    "            for n in range(n_slices_bounds[0], n_slices_bounds[1] + 1, 20)\n",
    "        ])\n",
    "        \n",
    "        # Ensure same column order\n",
    "        candidates = candidates[feature_cols]\n",
    "        \n",
    "        candidates[\"pred_score\"] = model.predict(candidates)\n",
    "        best = candidates.sort_values(\"pred_score\", ascending=False).iloc[0]\n",
    "        \n",
    "        # Evaluate best candidate\n",
    "        days = int(best[\"slice_length_days\"])\n",
    "        n_slices = int(best[\"n_slices\"])\n",
    "        \n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        \n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "        \n",
    "        print(f\"Iteration {it+1}/{n_iter} → days={days}, n_slices={n_slices}, score={score:.4f}\")\n",
    "    \n",
    "    results = pd.DataFrame(history)\n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Heatmap of normality scores across slice_length_days and n_slices.\n",
    "    \"\"\"\n",
    "    pivot = results.pivot_table(\n",
    "        index=\"slice_length_days\", columns=\"n_slices\", values=\"score\", aggfunc=\"mean\"\n",
    "    )\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Normality Score Heatmap\", fontsize=14)\n",
    "    plt.xlabel(\"n_slices\")\n",
    "    plt.ylabel(\"slice_length_days\")\n",
    "    plt.imshow(pivot, aspect=\"auto\", cmap=\"viridis\", origin=\"lower\")\n",
    "    plt.colorbar(label=\"Shapiro-Wilk p-value (Normality Score)\")\n",
    "    plt.xticks(range(len(pivot.columns)), pivot.columns, rotation=45)\n",
    "    plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = optimize_normality(\n",
    "    symbol, frontier_date, ending_date,\n",
    "    n_iter=20, init_points=5,\n",
    "    slice_length_bounds=(1, 15),\n",
    "    n_slices_bounds=(20, 200),\n",
    "    column=\"log_return\"\n",
    ")\n",
    "\n",
    "print(\"Best run:\")\n",
    "print(results_df.sort_values(\"score\", ascending=False).head(1))\n",
    "\n",
    "plot_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# --- Feature extraction ---\n",
    "def extract_features(df, column=\"log_return\"):\n",
    "    data = df[column].dropna()\n",
    "    if len(data) < 3:\n",
    "        return None, 0\n",
    "    features = {\n",
    "        \"mean\": np.mean(data),\n",
    "        \"std\": np.std(data),\n",
    "        \"skewness\": skew(data),\n",
    "        \"kurtosis\": kurtosis(data),\n",
    "        \"n_obs\": len(data),\n",
    "    }\n",
    "    _, p = shapiro(data)\n",
    "    return features, p\n",
    "\n",
    "# --- Optimizer ---\n",
    "def optimize_normality(symbol, frontier_date, ending_date, \n",
    "                       n_iter=50, init_points=10, batch_size=3,\n",
    "                       slice_length_bounds=(1, 20),\n",
    "                       n_slices_bounds=(20, 500),\n",
    "                       column=\"log_return\"):\n",
    "    \"\"\"\n",
    "    Closed-loop optimizer to maximize Shapiro-Wilk p-value (normality).\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    # Step 1: Initial exploration (random)\n",
    "    for _ in range(init_points):\n",
    "        days = random.randint(*slice_length_bounds)\n",
    "        n_slices = random.randint(*n_slices_bounds)\n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "    \n",
    "    # Step 2: Adaptive search\n",
    "    for it in range(init_points, n_iter):\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        X = hist_df.drop(columns=[\"score\"])\n",
    "        y = hist_df[\"score\"]\n",
    "        feature_cols = X.columns.tolist()\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Build candidate grid\n",
    "        candidates = pd.DataFrame([\n",
    "            {\"slice_length_days\": d, \"n_slices\": n,\n",
    "             \"mean\": 0, \"std\": 0, \"skewness\": 0, \"kurtosis\": 0, \"n_obs\": 0}\n",
    "            for d in range(slice_length_bounds[0], slice_length_bounds[1] + 1)\n",
    "            for n in range(n_slices_bounds[0], n_slices_bounds[1] + 1, 20)\n",
    "        ])\n",
    "        candidates = candidates[feature_cols]\n",
    "        candidates[\"pred_score\"] = model.predict(candidates)\n",
    "        \n",
    "        # Take top-k candidates for evaluation\n",
    "        topk = candidates.sort_values(\"pred_score\", ascending=False).head(batch_size)\n",
    "        \n",
    "        for _, row in topk.iterrows():\n",
    "            days = int(row[\"slice_length_days\"])\n",
    "            n_slices = int(row[\"n_slices\"])\n",
    "            random_slices = fetch_random_slices(\n",
    "                symbol, frontier_date, ending_date,\n",
    "                interval=Resolution.Minute,\n",
    "                slice_length=timedelta(days=days),\n",
    "                n_slices=n_slices\n",
    "            )\n",
    "            df = slices_to_extended_dataframe(random_slices)\n",
    "            features, score = extract_features(df, column=column)\n",
    "            if features:\n",
    "                features[\"slice_length_days\"] = days\n",
    "                features[\"n_slices\"] = n_slices\n",
    "                features[\"score\"] = score\n",
    "                history.append(features)\n",
    "        \n",
    "        print(f\"Iteration {it+1}/{n_iter} complete. Best score so far: {max(h['score'] for h in history):.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(history)\n",
    "\n",
    "# --- 3D Surface Plot ---\n",
    "def plot_surface(results):\n",
    "    \"\"\"\n",
    "    3D surface plot of slice_length_days × n_slices vs normality score.\n",
    "    \"\"\"\n",
    "    pivot = results.pivot_table(\n",
    "        index=\"slice_length_days\", columns=\"n_slices\", values=\"score\", aggfunc=\"mean\"\n",
    "    )\n",
    "    X, Y = np.meshgrid(pivot.columns, pivot.index)\n",
    "    Z = pivot.values\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"k\", alpha=0.8)\n",
    "    \n",
    "    ax.set_title(\"Normality Score Surface\", fontsize=14)\n",
    "    ax.set_xlabel(\"n_slices\", fontsize=12)\n",
    "    ax.set_ylabel(\"Slice Length (days)\", fontsize=12)\n",
    "    ax.set_zlabel(\"Shapiro-Wilk p-value\", fontsize=12)\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label=\"p-value\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = optimize_normality(\n",
    "    symbol, frontier_date, ending_date,\n",
    "    n_iter=3, init_points=4, batch_size=1,\n",
    "    slice_length_bounds=(1, 180),\n",
    "    n_slices_bounds=(1, 500),\n",
    "    column=\"log_return\"\n",
    ")\n",
    "\n",
    "print(\"Best run:\")\n",
    "print(results_df.sort_values(\"score\", ascending=False).head(1))\n",
    "\n",
    "plot_surface(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# --- Feature extraction ---\n",
    "def extract_features(df, column=\"log_return\"):\n",
    "    data = df[column].dropna()\n",
    "    if len(data) < 3:\n",
    "        return None, 0\n",
    "    features = {\n",
    "        \"mean\": np.mean(data),\n",
    "        \"std\": np.std(data),\n",
    "        \"skewness\": skew(data),\n",
    "        \"kurtosis\": kurtosis(data),\n",
    "        \"n_obs\": len(data),\n",
    "    }\n",
    "    _, p = shapiro(data)\n",
    "    return features, p\n",
    "\n",
    "# --- Optimizer ---\n",
    "def optimize_normality(symbol, frontier_date, ending_date, \n",
    "                       n_iter=50, init_points=10, batch_size=3,\n",
    "                       slice_length_bounds=(1, 20),\n",
    "                       n_slices_bounds=(20, 500),\n",
    "                       column=\"log_return\",\n",
    "                       alpha=0.05):\n",
    "    \"\"\"\n",
    "    Closed-loop optimizer to maximize Shapiro-Wilk p-value (normality).\n",
    "    Returns:\n",
    "      - full history of all runs\n",
    "      - summary stats of non-significant runs (p <= alpha)\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    # Step 1: Initial exploration (random)\n",
    "    for _ in range(init_points):\n",
    "        days = random.randint(*slice_length_bounds)\n",
    "        n_slices = random.randint(*n_slices_bounds)\n",
    "        random_slices = fetch_random_slices(\n",
    "            symbol, frontier_date, ending_date,\n",
    "            interval=Resolution.Minute,\n",
    "            slice_length=timedelta(days=days),\n",
    "            n_slices=n_slices\n",
    "        )\n",
    "        df = slices_to_extended_dataframe(random_slices)\n",
    "        features, score = extract_features(df, column=column)\n",
    "        if features:\n",
    "            features[\"slice_length_days\"] = days\n",
    "            features[\"n_slices\"] = n_slices\n",
    "            features[\"score\"] = score\n",
    "            history.append(features)\n",
    "    \n",
    "    # Step 2: Adaptive search\n",
    "    for it in range(init_points, n_iter):\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        X = hist_df.drop(columns=[\"score\"])\n",
    "        y = hist_df[\"score\"]\n",
    "        feature_cols = X.columns.tolist()\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Build candidate grid\n",
    "        candidates = pd.DataFrame([\n",
    "            {\"slice_length_days\": d, \"n_slices\": n,\n",
    "             \"mean\": 0, \"std\": 0, \"skewness\": 0, \"kurtosis\": 0, \"n_obs\": 0}\n",
    "            for d in range(slice_length_bounds[0], slice_length_bounds[1] + 1)\n",
    "            for n in range(n_slices_bounds[0], n_slices_bounds[1] + 1, 20)\n",
    "        ])\n",
    "        candidates = candidates[feature_cols]\n",
    "        candidates[\"pred_score\"] = model.predict(candidates)\n",
    "        \n",
    "        # Take top-k candidates for evaluation\n",
    "        topk = candidates.sort_values(\"pred_score\", ascending=False).head(batch_size)\n",
    "        \n",
    "        for _, row in topk.iterrows():\n",
    "            days = int(row[\"slice_length_days\"])\n",
    "            n_slices = int(row[\"n_slices\"])\n",
    "            random_slices = fetch_random_slices(\n",
    "                symbol, frontier_date, ending_date,\n",
    "                interval=Resolution.Minute,\n",
    "                slice_length=timedelta(days=days),\n",
    "                n_slices=n_slices\n",
    "            )\n",
    "            df = slices_to_extended_dataframe(random_slices)\n",
    "            features, score = extract_features(df, column=column)\n",
    "            if features:\n",
    "                features[\"slice_length_days\"] = days\n",
    "                features[\"n_slices\"] = n_slices\n",
    "                features[\"score\"] = score\n",
    "                history.append(features)\n",
    "        \n",
    "        best_so_far = max(h['score'] for h in history)\n",
    "        print(f\"Iteration {it+1}/{n_iter} complete. Best score so far: {best_so_far:.4f}\")\n",
    "    \n",
    "    results = pd.DataFrame(history)\n",
    "    \n",
    "    # --- Summarize non-significant runs ---\n",
    "    non_sig = results[results[\"score\"] <= alpha]\n",
    "    if non_sig.empty:\n",
    "        summary = {\n",
    "            \"n_non_significant\": 0,\n",
    "            \"mean_p\": None,\n",
    "            \"var_p\": None,\n",
    "            \"avg_slice_length_days\": None,\n",
    "            \"avg_n_slices\": None\n",
    "        }\n",
    "    else:\n",
    "        summary = {\n",
    "            \"n_non_significant\": len(non_sig),\n",
    "            \"mean_p\": non_sig[\"score\"].mean(),\n",
    "            \"var_p\": non_sig[\"score\"].var(),\n",
    "            \"avg_slice_length_days\": non_sig[\"slice_length_days\"].mean(),\n",
    "            \"avg_n_slices\": non_sig[\"n_slices\"].mean()\n",
    "        }\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# --- 3D Surface Plot ---\n",
    "def plot_surface_with_points(results):\n",
    "    \"\"\"\n",
    "    3D surface plot of slice_length_days × n_slices vs normality score,\n",
    "    with scatter points for all actual runs.\n",
    "    \"\"\"\n",
    "    # Build pivot for surface\n",
    "    pivot = results.pivot_table(\n",
    "        index=\"slice_length_days\", columns=\"n_slices\", values=\"score\", aggfunc=\"mean\"\n",
    "    )\n",
    "    X, Y = np.meshgrid(pivot.columns, pivot.index)\n",
    "    Z = pivot.values\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    \n",
    "    # Surface (smoothed averages)\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"k\", alpha=0.6)\n",
    "    \n",
    "    # Scatter points (actual trials)\n",
    "    sc = ax.scatter(\n",
    "        results[\"n_slices\"], \n",
    "        results[\"slice_length_days\"], \n",
    "        results[\"score\"], \n",
    "        c=results[\"score\"], \n",
    "        cmap=\"viridis\", \n",
    "        s=40, \n",
    "        edgecolor=\"k\"\n",
    "    )\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_title(\"Normality Score Surface with Trial Points\", fontsize=14)\n",
    "    ax.set_xlabel(\"n_slices\", fontsize=12)\n",
    "    ax.set_ylabel(\"Slice Length (days)\", fontsize=12)\n",
    "    ax.set_zlabel(\"Shapiro-Wilk p-value\", fontsize=12)\n",
    "    \n",
    "    # Colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label=\"p-value\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_surface_with_points_and_path(results):\n",
    "    \"\"\"\n",
    "    3D surface plot of slice_length_days × n_slices vs normality score,\n",
    "    with scatter points for all actual runs and a line showing the optimizer's search path.\n",
    "    \"\"\"\n",
    "    # Build pivot for surface\n",
    "    pivot = results.pivot_table(\n",
    "        index=\"slice_length_days\", columns=\"n_slices\", values=\"score\", aggfunc=\"mean\"\n",
    "    )\n",
    "    X, Y = np.meshgrid(pivot.columns, pivot.index)\n",
    "    Z = pivot.values\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    \n",
    "    # Surface (smoothed averages)\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"k\", alpha=0.5)\n",
    "    \n",
    "    # Scatter points (actual trials)\n",
    "    sc = ax.scatter(\n",
    "        results[\"n_slices\"], \n",
    "        results[\"slice_length_days\"], \n",
    "        results[\"score\"], \n",
    "        c=results[\"score\"], \n",
    "        cmap=\"viridis\", \n",
    "        s=40, \n",
    "        edgecolor=\"k\"\n",
    "    )\n",
    "    \n",
    "    # Trajectory line (optimizer's path, in trial order)\n",
    "    ax.plot(\n",
    "        results[\"n_slices\"], \n",
    "        results[\"slice_length_days\"], \n",
    "        results[\"score\"], \n",
    "        color=\"red\", \n",
    "        linewidth=2, \n",
    "        alpha=0.8, \n",
    "        marker=\"o\", \n",
    "        markersize=3\n",
    "    )\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_title(\"Normality Score Surface with Trials & Search Path\", fontsize=14)\n",
    "    ax.set_xlabel(\"n_slices\", fontsize=12)\n",
    "    ax.set_ylabel(\"Slice Length (days)\", fontsize=12)\n",
    "    ax.set_zlabel(\"Shapiro-Wilk p-value\", fontsize=12)\n",
    "    \n",
    "    # Colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label=\"p-value\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, summary = optimize_normality(\n",
    "    symbol, frontier_date, ending_date,\n",
    "    n_iter=50, init_points=10, batch_size=3,\n",
    "    slice_length_bounds=(1, 15),\n",
    "    n_slices_bounds=(20, 200),\n",
    "    column=\"log_return\",\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"Full results (first few):\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(\"\\nSummary of Non-Significant Runs:\")\n",
    "print(summary)\n",
    "\n",
    "plot_surface(results_df)\n",
    "\n",
    "plot_surface_with_points_and_path(results_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
